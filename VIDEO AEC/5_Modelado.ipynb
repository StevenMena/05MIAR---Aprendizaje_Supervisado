{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0njkxZM12GMi",
        "outputId": "aeff1170-d713-4ad2-ab52-92036308161b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(152, 10)\n",
            "(152,)\n"
          ]
        }
      ],
      "source": [
        "# Cargar los datos de entrenamiento\n",
        "import numpy as np\n",
        "train = np.load('../final_features/train.npy')\n",
        "\n",
        "X_train = train[:,:-1]\n",
        "y_train = train[:, -1]\n",
        "\n",
        "print(np.shape(X_train))\n",
        "print(np.shape(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4ZRVvJ22GMw",
        "outputId": "45747775-e545-4dd0-d514-47171346145c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LOGR:  Accuracy:  0.8880 +/- 0.0174\n",
            "MLP:  Accuracy:  0.9146 +/- 0.0335\n"
          ]
        }
      ],
      "source": [
        "# Definir los modelos de clasificación\n",
        "from sklearn.linear_model import LogisticRegression #Dice Regression pero es un Algoritmo para ser usado por clasificación\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "#Estudiar estos dos algoritmos\n",
        "#random_state es la semilla\n",
        "#El número de capas o neuronas debe ser menor al número de características\n",
        "algoritmos = {'LOGR': LogisticRegression(penalty='l2', solver='saga', max_iter=1000, random_state=42),\n",
        "             'MLP': MLPClassifier(hidden_layer_sizes=[8,4], activation='relu', solver='sgd', batch_size='auto',\n",
        "                                 learning_rate='adaptive', learning_rate_init=0.01, max_iter=1000, random_state=42)}\n",
        "#hIPERPARAMETROS INICIALES\n",
        "#algoritmos = {'LOGR': LogisticRegression(penalty='l2', solver='lbfgs', max_iter=100, random_state=42),\n",
        "#             'MLP': MLPClassifier(hidden_layer_sizes=[100,], activation='relu', solver='adam', batch_size='auto',\n",
        "#                                 learning_rate='constant', learning_rate_init=0.001, max_iter=200, random_state=42)}\n",
        "\n",
        "\n",
        "# Cross-validation interno en k=5 bolsas\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "\n",
        "results={}\n",
        "for nombre, alg in algoritmos.items():\n",
        "    results[nombre] = cross_val_score(alg, X_train, y_train, cv=KFold(n_splits=5, shuffle=True, random_state=42))\n",
        "    print(nombre + ':  Accuracy:  %0.4f +/- %0.4f' % (results[nombre].mean(), results[nombre].std()))\n",
        "    #El porcentaje de número de datos que estoy acertando en las pediciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4nY7cUL2GMz",
        "outputId": "26586a8b-2f09-4483-a383-96486bad2b2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mínimo error cometido:  0.1579953781086285\n",
            "Número de iteraciones llevadas a cabo:  600\n"
          ]
        }
      ],
      "source": [
        "# Definimos el modelo definitivo.\n",
        "algoritmos = {'LOGR': LogisticRegression(penalty='l2', solver='saga', max_iter=1000, random_state=42),\n",
        "             'MLP': MLPClassifier(hidden_layer_sizes=[8,4], activation='relu', solver='sgd', batch_size='auto',\n",
        "                                 learning_rate='adaptive', learning_rate_init=0.01, max_iter=1000, random_state=42)}\n",
        "#Entrenamos todos los datos luego de mejorar nuestros algoritmos\n",
        "LOGR_definitivo = LOGR.fit(X_train, y_train)\n",
        "MLP_definitivo = MLP.fit(X_train, y_train)\n",
        "\n",
        "# Atributos se obtienen durante el entrenamiento\n",
        "#Sirven para utilizar cuando se tienen más datos y es necesario ajustar los algoritmos de entrenamiento\n",
        "#Estos como son de MLP servirian para el alg MLP\n",
        "print('Mínimo error cometido: ', MLP_definitivo.best_loss_)\n",
        "#Conocer cuantas iteraciones necesita el entrenamiento\n",
        "print('Número de iteraciones llevadas a cabo: ', MLP_definitivo.n_iter_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGyv8Bxo2GM2"
      },
      "outputs": [],
      "source": [
        "# Guardar modelos\n",
        "import os\n",
        "if not os.path.exists('../models'):\n",
        "    os.mkdir('../models')\n",
        "    \n",
        "import pickle\n",
        "with open('../models/LOGR.pickle', 'wb') as fw:\n",
        "    pickle.dump(LOGR_definitivo, fw)\n",
        "with open('../models/MLP.pickle', 'wb') as fw:\n",
        "    pickle.dump(MLP_definitivo, fw)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}