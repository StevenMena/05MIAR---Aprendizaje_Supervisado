{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StevenMena/05MIAR---Aprendizaje_Supervisado/blob/main/Examen_C2_MenaChavez_StevenAlberto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naRZ2DVSqxX3"
      },
      "source": [
        "### EXAMEN - Convocatoria 2 - Programación\n",
        "Utilizar el conjunto de datos \"dataset_exam_C2.csv\" para resolver el ejercicio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DxK0439qxYF"
      },
      "source": [
        "#### 1) Carga de datos (0.5 puntos)\n",
        "Cargar el dataframe \"dataset_exam_C2.csv\" y obtener por separado los datos \"X\" y la variable target \"y\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.feature_selection import SelectPercentile, mutual_info_regression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import make_scorer, accuracy_score,  precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "O9YCScwJsW2F"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAs2sFfMqxYc"
      },
      "outputs": [],
      "source": [
        "# Cargar el archivo \"dataset_exam_C2.csv\" utilizando la librería pandas\n",
        "\n",
        "\n",
        "# Convertir la información en formato numpy.array()\n",
        "\n",
        "\n",
        "# Separar los datos \"X\" y la variable target \"y\" (el target corresponde a la última columna)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Para cargar el archivo en formato npy\n",
        "data = np.load('dataset_exam_C2_backup.npy', allow_pickle=True)\n",
        "#display(data)\n",
        "\n",
        "# Separar los datos \"X\" y la variable target \"y\" (el target corresponde a la última columna)\n",
        "\n",
        "X =  data[:,:-1]\n",
        "y = data[:,-1]"
      ],
      "metadata": {
        "id": "jqY2Mgi3rwJn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v-nlzQPqxYk"
      },
      "source": [
        "#### 2) Tratamiento de datos (1 punto)\n",
        "¡IMPORTANTE! Si este ejercicio no se consigue resolver, pasad directamente al siguiente cargando el dataset correspondiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WP6kfiycqxYn"
      },
      "outputs": [],
      "source": [
        "# Discretizar los atributos relativos a variables categóricas, es decir, no numéricas. \n",
        "# Se recomienda utilizar la función \"isinstance()\" para saber si la variable es numérica o no.\n",
        "# Se recomienda utilizar la función preprocessing.OrdinalEncoder() + \"fit_transform() para realizar directamente la conversión.\n",
        "# Otras funciones que pueden ayudar para tratar la dimensión de los datos son: \"np.expand_dims()\" y \"np.squeeze()\"\n",
        "\n",
        "\n",
        "# Mostrar por pantalla cómo queda el dataset tras discretizar las variables categóricas\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brpO9yQmqxYp"
      },
      "source": [
        "#### 3) Partición de datos externa (1 punto)\n",
        "¡IMPORTANTE!  En caso de no haber realizado con éxito el ejercicio anterior, cargar primero el \"dataset_backup.npy\" utilizando \"np.load()\". Después, vuelve a separar los datos \"X\" y la variable target \"y\".\n",
        "\n",
        "Realizar una partición externa de tipo hold-out seleccionando un 20% de los datos para test (fijar una semilla en 42)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHEga391qxYr",
        "outputId": "2ee70f5f-d155-4593-807b-0adafed8b1b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de datos de Entrenamiento:  (1070, 6)\n",
            "Matriz de datos de Test:  (268, 6)\n"
          ]
        }
      ],
      "source": [
        "# Partición hold-out seleccionado un 20% de los datos para test. Fijar semilla en 42.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "# Mostrar por pantalla las dimensiones de los datos de train y test\n",
        "print(\"Matriz de datos de Entrenamiento: \", np.shape(X_train))\n",
        "print(\"Matriz de datos de Test: \", np.shape(X_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhYAerSHqxYt"
      },
      "source": [
        "#### 4) Estandarización de los datos de train y test (1 punto)\n",
        "Utilizar el método StandardScaler()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zhNucRtbqxYv"
      },
      "outputs": [],
      "source": [
        "standardizer = StandardScaler()\n",
        "\n",
        "# Estandarización de los datos de entrenamiento\n",
        "X_train_std = standardizer.fit_transform(X_train)\n",
        "\n",
        "# Estandarización de los datos de test\n",
        "X_test_std = standardizer.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkoLD5u-qxYy"
      },
      "source": [
        "#### 5) Selección de atributos en train y test (1 punto)\n",
        "Aplicar el método de mutual_info_regression utilizando la función \"SelectPercentile\" con percentile=90"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi283yYXqxYz",
        "outputId": "40d0dcd1-6610-4ad5-969e-4117c47b7c5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nueva Matriz de datos de Entrenamiento:  (1070, 5)\n",
            "Nueva Matriz de datos de Test:  (268, 5)\n"
          ]
        }
      ],
      "source": [
        "selector = SelectPercentile(mutual_info_regression, percentile=90)\n",
        "# Selección de los atributos de entrenamiento\n",
        "X_train_selected = selector.fit_transform(X_train_std, y_train)\n",
        "\n",
        "# Selección de los atributos de test\n",
        "X_test_selected = selector.transform(X_test_std)\n",
        "\n",
        "# Mostrar por pantalla las dimensiones de los datos de train y test\n",
        "print(\"Nueva Matriz de datos de Entrenamiento: \", np.shape(X_train_selected))\n",
        "print(\"Nueva Matriz de datos de Test: \", np.shape(X_test_selected))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjmnccW4qxY0"
      },
      "source": [
        "#### 6) Comparación de modelos de regresión mediante validación cruzada (2.5 puntos)\n",
        "Aplicar una validación cruzada interna de K=5 bolsas para optimizar y comparar la capacidad predictiva de los siguientes modelos: K-vecinos más cercanos (KNN) y Support Vector Machine (SVM). La comparación debe realizarse únicamente en términos de coeficiente de determinación (R2) proporcionando resultados de media +- desviación estándar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "id": "8ZPClQS6qxY1",
        "outputId": "48c19d99-325a-47b2-955c-7910f874e373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cross_val_R2_K-Vecinos:   0.8142 +/- 0.0279\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-0064ee21ccdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mr2_cv_results_svm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_selected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cross_val_R2_SVM:   %0.4f +/- %0.4f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr2_cv_results_svm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2_cv_results_svm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    516\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    283\u001b[0m     )\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m     \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# For callabe scoring, the return type is only know after calling. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             )\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py\", line 180, in fit\n    self._validate_params()\n  File \"/usr/local/lib/python3.9/dist-packages/sklearn/base.py\", line 600, in _validate_params\n    validate_parameter_constraints(\n  File \"/usr/local/lib/python3.9/dist-packages/sklearn/utils/_param_validation.py\", line 97, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'kernel' parameter of SVC must be a str among {'linear', 'rbf', 'poly', 'precomputed', 'sigmoid'} or a callable. Got 'lineal' instead.\n"
          ]
        }
      ],
      "source": [
        "#Definir K-vecinos\n",
        "k = 10\n",
        "reg = KNeighborsRegressor(n_neighbors = k, weights='distance', metric='euclidean')\n",
        "\n",
        "#Definir SVM\n",
        "svm = SVC(C=1, kernel='rbf')\n",
        "\n",
        "r2_cv_results = cross_val_score(reg, X_train_selected, y_train, cv = KFold(n_splits=5, shuffle=True, random_state=42), scoring='r2')\n",
        "print(\"cross_val_R2_K-Vecinos:   %0.4f +/- %0.4f\" % (r2_cv_results.mean(), r2_cv_results.std()))\n",
        "\n",
        "\n",
        "r2_cv_results_svm = cross_val_score(svm, X_train_selected, y_train, cv = KFold(n_splits=5, shuffle=True, random_state=42), scoring='r2')\n",
        "print(\"cross_val_R2_SVM:   %0.4f +/- %0.4f\" % (r2_cv_results_svm.mean(), r2_cv_results_svm.std()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cma93eNqxY2"
      },
      "source": [
        "#### 7) Evaluación de los modelos sobre el conjunto de test (1 punto)\n",
        "- Definir una función \"plot_bisectriz()\" cuyos inputs sean:\n",
        " * \"y\" --> la variable target,\n",
        " * \"y_pred\" --> las predicciones del modelo, y \n",
        " * \"name\" --> el nombre del algoritmo utilizado.\n",
        " \n",
        "- El output de dicha función será un gráfico basado en la bisectriz para ver las sobreestimaciones y subestimaciones de cada modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2NPe4jxqxY3"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRXJWVxVqxY4"
      },
      "source": [
        "#### 8) Evaluación de los modelos sobre el conjunto de test (2 puntos)\n",
        "- Entrenar los modelos anteriores utilizando todos los datos de entrenamiento.\n",
        "- Evaluar su rendimiento sobre el conjunto de test.\n",
        "- Llamar a la función \"plot_bisectriz\" para ver las sobreestimaciones y subestimaciones de cada modelo.\n",
        "- Crear una tabla donde se muestren los resultados de todos los modelos.\n",
        " * Las filas serán: R2, MAE, MAPE, MSE y RMSE\n",
        " * Las columnas serán: KNN y SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9zXLz6OqxY5"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}